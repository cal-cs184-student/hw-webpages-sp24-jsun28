<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Arjun Damerla and Julia Sun</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/hw-webpages-sp24-jsun28/">Webpage Link</a></h2>

<br><br>

<div>

<h2 align="middle">Overview</h2>
<p>
  This project was a way to help us further understand the development of the rendering pipeline. Specifically, in each part of this project, we learned about a different topic: <br><br>
  In part 1, we learned about different intersection algorithms and how to implement them, as well as ray generation. <br><br>
  In part 2, we learned how to run these intersection tests we learned in part 1 MUCH quicker, resulting in much faster results, aka faster rendering. <br><br>
  In part 3, using ideas like the Monte Carlo Estimator, we render images with much more realistic shading, and also learn about different materials that both take in and also reflect out light in different directions.  <br><br>
  In part 4, we implemented sampling with diffuse bsdf, as well as adding some indirect lighting effects, using recursion as well as the Russian Roulette idea <br><br>
  Finally, in part 5 we implemented adaptive sampling, allowing us to decrease the amount of noise in our renders. <br><br>
  
  This project was very fun! We were able to understand the processes behind illumination and ray tracer methods, and although we weren’t able to fully achieve the results we wanted near the end (see part 4 and 5), we were still able to implement a rather successful renderer for these complex objects!  <br><br>
  
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<p>
  To generate a ray, we had to first complete the Camera::generate_ray(...) function which takes in two double coordinates representing the normalized image coordinates. To convert these coordinates into a ray in world space, we had to first convert the image coordinates to camera space. To do this, we first found the ratio for the width and height of the image when converting from image space to camera space. <br><br>
  Since the origin of the image space (0, 0) correlates to the origin point (-tan(0.5 * hFov), -tan(0.5 *vHov), -1) in the camera space, we found the ratio for the width to be 2 * tan(0.5 * hFov) and the ratio for the height to be 2 * tan(0.5 * vHov) where hFov and vHov are in radians. From there, we calculated the new_x and new_y coordinates in the camera space where new_x = (x - 0.5) * 2 * tan(0.5 * hFov) and the new_y coordinate = (y - 0.5) * 2 * tan(0.5 * vHov). <br><br>
  Using the new_x and new_y coordinates, we generated a new 3D vector (new_x, new_y, -1) which we multiplied with the c2w matrix and normalized the resulting vector afterwards. We then used our new normalized vector and the camera’s position in the world space to generate the final ray in the world space. Once the ray was created, we initialized the ray’s max_t and min_t to fClip and nClip respectively. <br><br>
  
  As for the triangle intersection algorithm, the main algorithm we implemented was the Moller Trumbore algorithm, which effectively finds whether or not a ray INTERSECTS a triangle in a 3 dimensional space. Mathematically, the algorithm finds a solution to <br><br>
  
  O + tD = (1 - b1 - b2) P0 + b1P1 + b2P2 <br><br>
  
  Where the ray has origin O and direction d, the triangle is defined with the vertices P0 P1 and P2, and we wish to find t, b1, and b2 that makes the equation true, where t is the distance along the ray, and b1, b2, and (1 - b1 - b2)  is where the ray intersects the triangle’s plane. <br><br>
  
  There are really two parts to this algorithm. Firstly, we need to make sure that the ray is NOT parallel to the plane of the triangle, because otherwise there will be no intersection. Afterwards, when the ray INTERSECTS the plane of the triangle, we need to make sure that the three values, (b1, b2, 1 - b1 - b2), are all between 0 and 1, as that would indicate that the ray intersects the plane of the triangle as WELL as inside of the triangle.<br><br>
  
</p>
<br>

<h3>
  Example of images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part1_blob.png" align="middle" width="400px"/>
        <figcaption>blob.dae</figcaption>
      </td>
      <td>
        <img src="part1_bench.png" align="middle" width="400px"/>
        <figcaption>bench.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part1_bunny.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<p>
  For the BVH construction algorithm, the main idea was that we implemented the algorithm in a recursive manner. In other words, when given a list of primitives such that the size of this list is MORE than max_leaf_size, we would then need to split this list such that these splits result in lists of all sizes at most max_leaf_size. As for our base case, when we have a list of primitives such that there are at most max_leaf_size primitives, we can then create a new node from the BVHNode class, and set its l and r pointers to NULL (there aren’t any left or right parts in the base case!), and we can then return this new leaf node. <br><br>

  In terms of our heuristic for finding the splits, this had to do with determining which of the x, y, or z axis we wanted to split. This led to us computing the average of the centroid values for each of the axes. From there, we selected the minimum of the number of primitives in each child node after splitting multiplied by the actual surface area of the child node’s bounding boxes after splitting, thus letting us know which axis to split on. <br><br>
  
</p>

<h3>
  A few images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part2_maxplanx.png" align="middle" width="400px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
      <td>
        <img src="part2_peter.png" align="middle" width="400px"/>
        <figcaption>peter.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part2_person.png" align="middle" width="400px"/>
        <figcaption>CBlucy.dae</figcaption>
      </td>
      <td>
        <img src="part2_cow.png" align="middle" width="400px"/>
        <figcaption>cow.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
  Without BVH acceleration, the rendering time for cow.dae took 9.99 seconds. However, after implementing BVH acceleration, the rendering time significantly decreased to 0.0388 seconds. Similarly, for CBluncy.dae, its rendering time before BVH acceleration was 24.1 seconds and after BVH acceleration was 0.0388 seconds. The BVH acceleration decreased the rendering time for CBluncy.dae at a higher rate than cow.dae.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<p>
  Before we can delve into multiple-bounce light rays, we first need to implement and understand zero bounce illumination (light going directly from the source to the camera), and one bounce illumination (when the light reflects exactly once on the scene). <br><br>

  In terms of implementation, the two ways we implemented direct lighting was through Importance sampling and Uniform Hemisphere Sampling. The main difference between these two implementations is the way in which they sample from the light sources. <br><br>
  
  More specifically, for Uniform Hemisphere Sampling, we use the Monte Carlo estimate of the reflectance of the light on our scene. In other words, if r_i was the ray from our light source, and i_1 was the intersection of our ray with the scene, then we first sample uniformly some other ray r_2 from the normal hemisphere of i_1 (this is why it’s called Uniform HEMISPHERE sampling). Assuming that i_1 actually intersects our scene (otherwise, don’t return any light), we now need to compute the following: <br><br>
  
  Set brdf = f(i_1, r_1 -> r_2), as f(i_1, r_1 -> r_2) is the bidirectional reflectance distribution function from the second ray r_2 to the first ray r_1. Then, radiance = L(i_1, r_2) will be the radiance along r_2 incident to i_1. Finally, pdf = p(r_2) is the probability density function of actually SAMPLING r_2 in the first place, and x_j is the angle between the normal surface intersection and r_2. With all of these variables, we calculate <br><br>
  (brdf * radiance * cos(x)) / pdf<br><br>
  
  Which is the light that is reflected from before our bounce to after our bounce (or in other words, from r_2 to r_1).<br><br>
  We ultimately will sum up all of these light values from each of these points, and then divide by the total number of light values we had, thus computing a good estimate of the amount of light we need.<br><br>
  
  Importance sampling is pretty similar, but as said before, the main difference is how we TREAT each sample in our Monte Carlo Estimator. The main philosophy behind importance sampling is that we sample light PROPORTIONAL to how much we expect that light to make a difference to our scene. So, the same calculations are performed as in hemisphere sampling, but now we will divide our calculation for some light by the probability that that sample is actually SAMPLED.<br><br>
  
</p>

<h3>
 Some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="part3_hemi2.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="part3_imp2.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="part3_hemi3.png" align="middle" width="400px"/>
        <figcaption>CBcoil.dae</figcaption>
      </td>
      <td>
        <img src="part3_imp3.png" align="middle" width="400px"/>
        <figcaption>CBcoil.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Dragon.dae rendered with 1, 4, 16, and 64 light rays and 1 sample per pixel using light sampling.
</h3>

<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="dragon_31.png" align="middle" width="400px"/>
        <figcaption>1 Light Ray (dragon.dae)</figcaption>
      </td>
      <td>
        <img src="dragon_34.png" align="middle" width="400px"/>
        <figcaption>4 Light Rays (dragon.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="dragon_316.png" align="middle" width="400px"/>
        <figcaption>16 Light Rays (dragon.dae)</figcaption>
      </td>
      <td>
        <img src="dragon_364.png" align="middle" width="400px"/>
        <figcaption>64 Light Rays (dragon.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    In dragon.dae, it is evident how the noise levels in soft shadows decreases as the number of samples per area light increases. When l=4, the noise level begins to decrease which allows the soft shadows in the dragon become more prominent against the black background. This pattern continues which can be shown when l=64, the noise level has decreased significantly enough to where the soft shadow of the dragon creates a much cleaner and more precise image of the dragon as detailed features on the dragon become more clearly visible.  
</p>
<br>

<p>
  From looking at the two images, it is evident that uniform hemisphere sampling has a lot more noise than lighting sampling. This is because for uniform hemisphere sampling, there is a higher chance that more of the rays sampled will not land on a light source as it takes samples from uniform directions in the hemisphere. However, in lighting sampling, it takes samples of rays that have a higher probability of pointing at a light source. Th0.erefore, lighting sampling renders a smoother and cleaner image in comparison to uniform hemisphere sampling. 
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<p>
  For this part, we implemented global bounce illumination, an indirect lighting method. This essentially allows us to create more light by taking into account more and more bounces of light (not just the first bounce). So, if we wanted to implement n-bounce sampling, we need to implement this function in a very recursive manner. <br><br>

  The main idea is that to implement n-bounce sampling, we sample a ray and perform one-bounce-illumination, and then recursively call n-1 bounce sampling on this new ray. We recurse on and on and on until eventually, the depth reaches 0, in which case we end our recursion. <br><br>
  
  In other words, we are performing one_bounce_radiance on the nth, n-1 th, n-2nd, … 3rd, 2nd, and 1st bounces, and add all of these light values up (assuming isAccumBounce is set to True), or just the light at the nth bounce (assuming isAccumBounce is set to False).<br><br>
  
  Another layer that can get added on to indirect lighting is the idea of Russian Roulette, which will allow us to randomly terminate during recursion. We implement this by using the method coin_flip(0.3), and we recurse whether or not this method returns true or not (probabilistic chance). The reason we implement this is due to the fact that close-to-infinite recursion is infeasible, as very late recursive calls in this case will add very very little, so they might as well not be added at all to save space and time. <br><br>
  
</p>
<br>

<h3>
  Some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part41_bench.png" align="middle" width="400px"/>
        <figcaption>bench.dae</figcaption>
      </td>
      <td>
        <img src="part41_dragon.png" align="middle" width="400px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Rendered views of CBbunny.dae with only direct illumination, then only indirect illumination. 
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part4t1.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="part4_indirect.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<br>

<h3>
  CBbunny.dae, comparing rendered views with max_ray_depth set to 0, 1, 2, 3, and 100.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part4_black.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="part4t1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part4t2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="part4t3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part4t4.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="part4_100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  When isAccumBounces=false, the light source becomes smaller as the max_ray_depth increases because for each time the light bounces, only a portion of its light is kept for the next depth. Therefore, the image rendered for when max_ray_depth=3 is darker than the image for when max_ray_depth=2 because when the light bounces, some of its original light is lost. Therefore, it is necessary to accumulate the light from each bounce in order to produce and retain a bright image. <br><br>
  When isAccumBounces=true, as the max_ray_depth increases, the amount of light should also increase as it recursively renders the original light from the previous depth as well as the new light that is bounced off in the new depth. However, when trying to implement at_least_one_bounce_radiance, we had an issue with our recursive call to one_bounce_radiance. We believe that in our recursive call, we were not able to add the correct amount of light from the previous bounce using one_bounce_radiance. This resulted in our image adding too much light with every bounce which ultimately rendered an image that was too bright. <br><br>
  
</p>
<br>

<h3>
  Rendered images of CBspheres_lambertian.dae with sample-per-pixel rates of 1, 2, 4, 8, 16, 64, and 1024. Uses 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part4sp1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="part4sp2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part4sp4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="part4sp8.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part4sp16.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="part4sp64.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part4sp1024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<p>
  In all of the previous parts, the main problem was the fact that there was still a good amount of noise in the scenes. So, instead of just increasing the number of samples per pixel (which will increase our space and time incredibly), we implement adaptive sampling, which is the idea of having a HIGHER sampling rate in areas of a scene that will converge slower, and having a LOWER sampling rate in areas of a scene that will converge faster. In our implementation, we first make two variables and set them both equal to 0. Then after sampling light in every iteration for a scene, we increment the first value by the light, and we increment the second value by the light squared. In other words, the first value will be the sum of lights, and the second value will be the sum of squares of lights. Then, we calculate this new value “I”, which will be equal to 1.96 * (standard deviation of the luminance of the samples / sqrt(number of samples so far)). This “I” value can be used to test whether or not we have successfully “converged” for a certain portion of the scene. In fact, the way we check convergence is by seeing if <br><br>

  I <= maxTolerance * mu <br><br>
  
  Where mu is equal to the mean of the light samples. Speaking of which, to calculate mu, we simply compute the first value divided by the number of samples, and to calculate the standard deviation, we compute (1/(num samples - 1)) * (the second value - ( first value squared / num samples)) (from the spec). <br><br>
  
  This maxTolerance could be actually made smaller to allow for a tighter convergence bound, or larger for a more loose convergence bound.<br><br>
  
</p>
<br>

<h3>
  CBbunny.dae and CBspheres_lambertian.dae rendered with at least 2048 samples per pixel with both of its rendered and sampling rate image. 
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part5b1.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="part5b2.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part5sp1.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="part5sp2.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
  Because we had an error in rendering the correct image in part 4, the final rendered image for adaptive sampling is also incorrect. Due to an error in our recursive call in at_least_one_bounce_radiance, the rendered image for both CBbunny.dae and CBspheres_lambertian.dae are too bright. Therefore, the sample rate image for both CBbunny.dae and CBspheres_lambertian.dae is also too bright. However, we believe our algorithm for part 5 is correct, but unfortunately due to our error in part 4, the final sample rate images are not correct.  
</p>


</body>
</html>
